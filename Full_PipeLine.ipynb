{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b522ce5a",
   "metadata": {},
   "source": [
    "# Flow:\n",
    "\n",
    "1) Read Data from folder\n",
    "2) Create Graphs & Labels and seperate into test, train & validation (Set1A, Set1B, Set 1C)\n",
    "4) Train GCN & DCGNN Models on the same (Model1 & Model2)\n",
    "5) Evaluate the model on Set1A \n",
    "6) Obtain Unlabelled graphs\n",
    "7) Create labels for these unlabelled graphs using trained model (Set2)\n",
    "8) Combine these trainable datasets as one complete SetC (Set1A, Set1B & Set 2)\n",
    "9) Train a new model1 with this combined data SetC\n",
    "10) Evaluate the model1 (trained new) on Set1A to check if performance has improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59bc4743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import stellargraph as sg\n",
    "from stellargraph.mapper import PaddedGraphGenerator\n",
    "from stellargraph.layer import GCNSupervisedGraphClassification\n",
    "from stellargraph.layer import DeepGraphCNN\n",
    "from stellargraph import StellarGraph\n",
    "\n",
    "from stellargraph import datasets\n",
    "\n",
    "from sklearn import model_selection\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Conv1D, MaxPool1D, Dropout, Flatten\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import createGraph\n",
    "import dataCreate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfffb9c",
   "metadata": {},
   "source": [
    "# Step 1) &  Step 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c144f587",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = 'graphsmallds/'\n",
    "dtrain_graphs,dtrain_labels = createGraph.readLabeledFolder(train_folder)\n",
    "\n",
    "#val_folder = ''\n",
    "#val_graphs,val_labels = createGraph.readLabeledFolder(train_folder)\n",
    "\n",
    "#unlabeled_folder = ''\n",
    "#unlabeled_graphs = createGrapg.readUnlabeledFolder(unlabeled_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d558a9",
   "metadata": {},
   "source": [
    "# Splitting & Training Data Step 3) & 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5513b4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper Parameters \n",
    "epochs = 200  # maximum number of training epochs\n",
    "folds = 2  # the number of folds for k-fold cross validation\n",
    "n_repeats = 5  # the number of repeats for repeated k-fold cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972956c8",
   "metadata": {},
   "source": [
    "### GCN Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfc32b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluating on fold 1 out of 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-20 19:26:54.482825: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluating on fold 2 out of 10...\n",
      "Training and evaluating on fold 3 out of 10...\n",
      "Training and evaluating on fold 4 out of 10...\n",
      "Training and evaluating on fold 5 out of 10...\n",
      "Training and evaluating on fold 6 out of 10...\n",
      "Training and evaluating on fold 7 out of 10...\n",
      "Training and evaluating on fold 8 out of 10...\n",
      "Training and evaluating on fold 9 out of 10...\n",
      "Training and evaluating on fold 10 out of 10...\n"
     ]
    }
   ],
   "source": [
    "def create_graph_classification_model(generator):\n",
    "    gc_model = GCNSupervisedGraphClassification(\n",
    "        layer_sizes=[64, 64],\n",
    "        activations=[\"relu\", \"relu\"],\n",
    "        generator=generator,\n",
    "        dropout=0.5,\n",
    "    )\n",
    "    x_inp, x_out = gc_model.in_out_tensors()\n",
    "    predictions = Dense(units=32, activation=\"relu\")(x_out)\n",
    "    predictions = Dense(units=16, activation=\"relu\")(predictions)\n",
    "    predictions = Dense(units=1, activation=\"sigmoid\")(predictions)\n",
    "\n",
    "    # Let's create the Keras model and prepare it for training\n",
    "    model = Model(inputs=x_inp, outputs=predictions)\n",
    "    model.compile(optimizer=Adam(0.005), loss=binary_crossentropy, metrics=[\"acc\"])\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_fold(model, train_gen, test_gen, es, epochs):\n",
    "    history = model.fit(\n",
    "        train_gen, epochs=epochs, validation_data=test_gen, verbose=0, callbacks=[es],\n",
    "    )\n",
    "    # calculate performance on the test data and return along with history\n",
    "    test_metrics = model.evaluate(test_gen, verbose=0)\n",
    "    test_acc = test_metrics[model.metrics_names.index(\"acc\")]\n",
    "\n",
    "    return history, test_acc\n",
    "\n",
    "def get_generators(train_index, test_index, graph_labels, batch_size):\n",
    "    train_gen = generator.flow(\n",
    "        train_index, targets=graph_labels.iloc[train_index].values, batch_size=batch_size\n",
    "    )\n",
    "    test_gen = generator.flow(\n",
    "        test_index, targets=graph_labels.iloc[test_index].values, batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    return train_gen, test_gen\n",
    "\n",
    "generator = PaddedGraphGenerator(graphs=dtrain_graphs)\n",
    "\n",
    "es = EarlyStopping(\n",
    "    monitor=\"val_loss\", min_delta=0, patience=25, restore_best_weights=True\n",
    ")\n",
    "\n",
    "test_accs = []\n",
    "\n",
    "#Create 10 fold training\n",
    "stratified_folds = model_selection.RepeatedStratifiedKFold(\n",
    "    n_splits=folds, n_repeats=n_repeats\n",
    ").split(dtrain_labels, dtrain_labels)\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(stratified_folds):\n",
    "    print(f\"Training and evaluating on fold {i+1} out of {folds * n_repeats}...\")\n",
    "    train_gen, test_gen = get_generators(\n",
    "        train_index, test_index, dtrain_labels, batch_size=30\n",
    "    )\n",
    "\n",
    "    model_1 = create_graph_classification_model(generator)\n",
    "\n",
    "    history, acc = train_fold(model_1, train_gen, test_gen, es, epochs)\n",
    "\n",
    "    test_accs.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78066473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy over all folds mean: 1e+02% and std: 0.0%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Count')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\n",
    "    f\"Accuracy over all folds mean: {np.mean(test_accs)*100:.3}% and std: {np.std(test_accs)*100:.2}%\"\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(test_accs)\n",
    "plt.xlabel(\"Accuracy\")\n",
    "plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfe4e12",
   "metadata": {},
   "source": [
    "### DGCNN Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "195d495b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/vihaan/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "WARNING:tensorflow:From /Users/vihaan/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py:629: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/1 [==============================] - 1s 742ms/step - loss: 0.6701 - acc: 0.6667 - val_loss: 0.6044 - val_acc: 1.0000\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.6102 - acc: 1.0000 - val_loss: 0.5111 - val_acc: 1.0000\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.4906 - acc: 1.0000 - val_loss: 0.3942 - val_acc: 1.0000\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.3967 - acc: 1.0000 - val_loss: 0.2737 - val_acc: 1.0000\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3332 - acc: 1.0000 - val_loss: 0.1683 - val_acc: 1.0000\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.2555 - acc: 1.0000 - val_loss: 0.0868 - val_acc: 1.0000\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.1915 - acc: 1.0000 - val_loss: 0.0393 - val_acc: 1.0000\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.2277 - acc: 1.0000 - val_loss: 0.0146 - val_acc: 1.0000\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.1536 - acc: 1.0000 - val_loss: 0.0046 - val_acc: 1.0000\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0997 - acc: 1.0000 - val_loss: 0.0012 - val_acc: 1.0000\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0789 - acc: 1.0000 - val_loss: 2.7255e-04 - val_acc: 1.0000\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0410 - acc: 1.0000 - val_loss: 5.3761e-05 - val_acc: 1.0000\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0420 - acc: 1.0000 - val_loss: 9.1070e-06 - val_acc: 1.0000\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0365 - acc: 1.0000 - val_loss: 1.3535e-06 - val_acc: 1.0000\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0110 - acc: 1.0000 - val_loss: 1.8643e-07 - val_acc: 1.0000\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0065 - acc: 1.0000 - val_loss: 2.6460e-08 - val_acc: 1.0000\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0106 - acc: 1.0000 - val_loss: 3.6749e-09 - val_acc: 1.0000\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0179 - acc: 1.0000 - val_loss: 4.6041e-10 - val_acc: 1.0000\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0046 - acc: 1.0000 - val_loss: 6.0699e-11 - val_acc: 1.0000\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0030 - acc: 1.0000 - val_loss: 7.6903e-12 - val_acc: 1.0000\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 6.2378e-04 - acc: 1.0000 - val_loss: 1.2436e-12 - val_acc: 1.0000\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 1.9169e-13 - val_acc: 1.0000\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 3.1432e-14 - val_acc: 1.0000\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 8.0527e-04 - acc: 1.0000 - val_loss: 5.6234e-15 - val_acc: 1.0000\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3.9383e-04 - acc: 1.0000 - val_loss: 9.6313e-16 - val_acc: 1.0000\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 4.5392e-04 - acc: 1.0000 - val_loss: 2.0046e-16 - val_acc: 1.0000\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 6.4446e-04 - acc: 1.0000 - val_loss: 5.1919e-17 - val_acc: 1.0000\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5.4767e-04 - acc: 1.0000 - val_loss: 1.2959e-17 - val_acc: 1.0000\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.3251e-04 - acc: 1.0000 - val_loss: 3.5360e-18 - val_acc: 1.0000\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 5.1360e-04 - acc: 1.0000 - val_loss: 1.0390e-18 - val_acc: 1.0000\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 9.5446e-05 - acc: 1.0000 - val_loss: 3.3138e-19 - val_acc: 1.0000\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 4.6563e-05 - acc: 1.0000 - val_loss: 1.1455e-19 - val_acc: 1.0000\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 6.0141e-04 - acc: 1.0000 - val_loss: 4.2155e-20 - val_acc: 1.0000\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.4041e-04 - acc: 1.0000 - val_loss: 1.6653e-20 - val_acc: 1.0000\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 6.4282e-05 - acc: 1.0000 - val_loss: 6.2483e-21 - val_acc: 1.0000\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2.6861e-04 - acc: 1.0000 - val_loss: 2.7948e-21 - val_acc: 1.0000\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2.2752e-04 - acc: 1.0000 - val_loss: 1.3221e-21 - val_acc: 1.0000\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 7.4054e-04 - acc: 1.0000 - val_loss: 6.4825e-22 - val_acc: 1.0000\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2.5414e-04 - acc: 1.0000 - val_loss: 3.3492e-22 - val_acc: 1.0000\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 4.9543e-05 - acc: 1.0000 - val_loss: 1.8222e-22 - val_acc: 1.0000\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 6.3356e-06 - acc: 1.0000 - val_loss: 1.0419e-22 - val_acc: 1.0000\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2.2181e-05 - acc: 1.0000 - val_loss: 6.2328e-23 - val_acc: 1.0000\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2.5482e-05 - acc: 1.0000 - val_loss: 3.8879e-23 - val_acc: 1.0000\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.3125e-04 - acc: 1.0000 - val_loss: 2.5071e-23 - val_acc: 1.0000\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4.4926e-05 - acc: 1.0000 - val_loss: 1.6749e-23 - val_acc: 1.0000\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.1138e-05 - acc: 1.0000 - val_loss: 1.1574e-23 - val_acc: 1.0000\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3.0396e-05 - acc: 1.0000 - val_loss: 8.2447e-24 - val_acc: 1.0000\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5.6290e-06 - acc: 1.0000 - val_loss: 6.0466e-24 - val_acc: 1.0000\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.6976e-06 - acc: 1.0000 - val_loss: 4.5558e-24 - val_acc: 1.0000\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3.1105e-05 - acc: 1.0000 - val_loss: 3.5134e-24 - val_acc: 1.0000\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.5438e-05 - acc: 1.0000 - val_loss: 2.7696e-24 - val_acc: 1.0000\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 7.4414e-05 - acc: 1.0000 - val_loss: 2.2224e-24 - val_acc: 1.0000\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2.5923e-05 - acc: 1.0000 - val_loss: 1.8154e-24 - val_acc: 1.0000\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4.6822e-05 - acc: 1.0000 - val_loss: 1.5054e-24 - val_acc: 1.0000\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.1144e-04 - acc: 1.0000 - val_loss: 1.2613e-24 - val_acc: 1.0000\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4.3864e-06 - acc: 1.0000 - val_loss: 1.0735e-24 - val_acc: 1.0000\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.5253e-04 - acc: 1.0000 - val_loss: 9.2066e-25 - val_acc: 1.0000\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.3266e-05 - acc: 1.0000 - val_loss: 7.9988e-25 - val_acc: 1.0000\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 8.5238e-08 - acc: 1.0000 - val_loss: 7.0386e-25 - val_acc: 1.0000\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3.9691e-06 - acc: 1.0000 - val_loss: 5.9217e-25 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.7954e-04 - acc: 1.0000 - val_loss: 5.2778e-25 - val_acc: 1.0000\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.5190e-06 - acc: 1.0000 - val_loss: 4.7529e-25 - val_acc: 1.0000\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3.5524e-05 - acc: 1.0000 - val_loss: 4.3102e-25 - val_acc: 1.0000\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 6.8313e-06 - acc: 1.0000 - val_loss: 3.9417e-25 - val_acc: 1.0000\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 4.0066e-07 - acc: 1.0000 - val_loss: 3.6342e-25 - val_acc: 1.0000\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3.3101e-06 - acc: 1.0000 - val_loss: 3.3749e-25 - val_acc: 1.0000\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 2.5474e-04 - acc: 1.0000 - val_loss: 3.1179e-25 - val_acc: 1.0000\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 6.6273e-06 - acc: 1.0000 - val_loss: 2.8998e-25 - val_acc: 1.0000\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.4549e-06 - acc: 1.0000 - val_loss: 2.7148e-25 - val_acc: 1.0000\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.1053e-05 - acc: 1.0000 - val_loss: 2.5549e-25 - val_acc: 1.0000\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.0241e-05 - acc: 1.0000 - val_loss: 2.4159e-25 - val_acc: 1.0000\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 6.5550e-07 - acc: 1.0000 - val_loss: 2.2962e-25 - val_acc: 1.0000\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.3184e-05 - acc: 1.0000 - val_loss: 2.1903e-25 - val_acc: 1.0000\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.1685e-06 - acc: 1.0000 - val_loss: 2.0983e-25 - val_acc: 1.0000\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 2.7244e-04 - acc: 1.0000 - val_loss: 1.9910e-25 - val_acc: 1.0000\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.0006e-05 - acc: 1.0000 - val_loss: 1.8972e-25 - val_acc: 1.0000\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3.6075e-05 - acc: 1.0000 - val_loss: 1.8116e-25 - val_acc: 1.0000\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.1380e-05 - acc: 1.0000 - val_loss: 1.7357e-25 - val_acc: 1.0000\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4.9036e-06 - acc: 1.0000 - val_loss: 1.6691e-25 - val_acc: 1.0000\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.0532e-05 - acc: 1.0000 - val_loss: 1.6093e-25 - val_acc: 1.0000\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.1097e-05 - acc: 1.0000 - val_loss: 1.5558e-25 - val_acc: 1.0000\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.5159e-06 - acc: 1.0000 - val_loss: 1.5087e-25 - val_acc: 1.0000\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.1047e-04 - acc: 1.0000 - val_loss: 1.4564e-25 - val_acc: 1.0000\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.1217e-05 - acc: 1.0000 - val_loss: 1.4095e-25 - val_acc: 1.0000\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.3276e-05 - acc: 1.0000 - val_loss: 1.3666e-25 - val_acc: 1.0000\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 7.6985e-05 - acc: 1.0000 - val_loss: 1.3221e-25 - val_acc: 1.0000\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.3768e-05 - acc: 1.0000 - val_loss: 1.2816e-25 - val_acc: 1.0000\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 7.8640e-06 - acc: 1.0000 - val_loss: 1.2453e-25 - val_acc: 1.0000\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5.3839e-06 - acc: 1.0000 - val_loss: 1.2128e-25 - val_acc: 1.0000\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.3432e-04 - acc: 1.0000 - val_loss: 1.1716e-25 - val_acc: 1.0000\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3.5423e-06 - acc: 1.0000 - val_loss: 1.1352e-25 - val_acc: 1.0000\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 2.3513e-06 - acc: 1.0000 - val_loss: 1.1029e-25 - val_acc: 1.0000\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2.1338e-06 - acc: 1.0000 - val_loss: 1.0742e-25 - val_acc: 1.0000\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.3670e-05 - acc: 1.0000 - val_loss: 1.0475e-25 - val_acc: 1.0000\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.1687e-05 - acc: 1.0000 - val_loss: 1.0229e-25 - val_acc: 1.0000\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4.2238e-07 - acc: 1.0000 - val_loss: 1.0011e-25 - val_acc: 1.0000\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.3112e-06 - acc: 1.0000 - val_loss: 9.8158e-26 - val_acc: 1.0000\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5.4131e-05 - acc: 1.0000 - val_loss: 9.6039e-26 - val_acc: 1.0000\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 9.6341e-06 - acc: 1.0000 - val_loss: 9.4080e-26 - val_acc: 1.0000\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3.7859e-05 - acc: 1.0000 - val_loss: 9.2080e-26 - val_acc: 1.0000\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 4.5238e-07 - acc: 1.0000 - val_loss: 9.0302e-26 - val_acc: 1.0000\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5.7738e-06 - acc: 1.0000 - val_loss: 8.8666e-26 - val_acc: 1.0000\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 7.6094e-05 - acc: 1.0000 - val_loss: 8.6671e-26 - val_acc: 1.0000\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.4214e-05 - acc: 1.0000 - val_loss: 8.4349e-26 - val_acc: 1.0000\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.9347e-07 - acc: 1.0000 - val_loss: 8.2686e-26 - val_acc: 1.0000\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4.2752e-06 - acc: 1.0000 - val_loss: 8.1173e-26 - val_acc: 1.0000\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.3114e-06 - acc: 1.0000 - val_loss: 7.9815e-26 - val_acc: 1.0000\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2.1788e-05 - acc: 1.0000 - val_loss: 7.8428e-26 - val_acc: 1.0000\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5.4539e-06 - acc: 1.0000 - val_loss: 7.7150e-26 - val_acc: 1.0000\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2.1230e-06 - acc: 1.0000 - val_loss: 7.5992e-26 - val_acc: 1.0000\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 7.1690e-05 - acc: 1.0000 - val_loss: 7.4580e-26 - val_acc: 1.0000\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5.4885e-06 - acc: 1.0000 - val_loss: 7.3279e-26 - val_acc: 1.0000\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4.4059e-06 - acc: 1.0000 - val_loss: 7.2091e-26 - val_acc: 1.0000\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.2831e-05 - acc: 1.0000 - val_loss: 7.0961e-26 - val_acc: 1.0000\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3.9151e-05 - acc: 1.0000 - val_loss: 6.9748e-26 - val_acc: 1.0000\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 6.6176e-06 - acc: 1.0000 - val_loss: 6.8617e-26 - val_acc: 1.0000\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.4808e-06 - acc: 1.0000 - val_loss: 6.7597e-26 - val_acc: 1.0000\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.6551e-06 - acc: 1.0000 - val_loss: 6.6675e-26 - val_acc: 1.0000\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 9.1729e-04 - acc: 1.0000 - val_loss: 6.2283e-26 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3.0813e-05 - acc: 1.0000 - val_loss: 5.8377e-26 - val_acc: 1.0000\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 7.1954e-06 - acc: 1.0000 - val_loss: 5.5019e-26 - val_acc: 1.0000\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 6.5130e-06 - acc: 1.0000 - val_loss: 5.2110e-26 - val_acc: 1.0000\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 9.2364e-06 - acc: 1.0000 - val_loss: 4.9561e-26 - val_acc: 1.0000\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.1732e-06 - acc: 1.0000 - val_loss: 4.7355e-26 - val_acc: 1.0000\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 7.6918e-05 - acc: 1.0000 - val_loss: 4.5135e-26 - val_acc: 1.0000\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.6894e-04 - acc: 1.0000 - val_loss: 4.2564e-26 - val_acc: 1.0000\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4.6994e-06 - acc: 1.0000 - val_loss: 4.0346e-26 - val_acc: 1.0000\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 2.0126e-06 - acc: 1.0000 - val_loss: 3.8430e-26 - val_acc: 1.0000\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 2.5487e-05 - acc: 1.0000 - val_loss: 3.6678e-26 - val_acc: 1.0000\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.8769e-05 - acc: 1.0000 - val_loss: 3.5103e-26 - val_acc: 1.0000\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5.7542e-06 - acc: 1.0000 - val_loss: 3.3716e-26 - val_acc: 1.0000\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2.4792e-04 - acc: 1.0000 - val_loss: 3.1801e-26 - val_acc: 1.0000\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5.5831e-06 - acc: 1.0000 - val_loss: 3.0146e-26 - val_acc: 1.0000\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.6282e-05 - acc: 1.0000 - val_loss: 2.8667e-26 - val_acc: 1.0000\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.9786e-04 - acc: 1.0000 - val_loss: 2.7029e-26 - val_acc: 1.0000\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 4.0053e-07 - acc: 1.0000 - val_loss: 2.5628e-26 - val_acc: 1.0000\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.0894e-05 - acc: 1.0000 - val_loss: 2.4392e-26 - val_acc: 1.0000\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 8.3432e-06 - acc: 1.0000 - val_loss: 2.3303e-26 - val_acc: 1.0000\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4.7965e-06 - acc: 1.0000 - val_loss: 2.2349e-26 - val_acc: 1.0000\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2.3008e-04 - acc: 1.0000 - val_loss: 2.1023e-26 - val_acc: 1.0000\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 2.4477e-05 - acc: 1.0000 - val_loss: 1.9834e-26 - val_acc: 1.0000\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 2.7306e-06 - acc: 1.0000 - val_loss: 1.8809e-26 - val_acc: 1.0000\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 6.7137e-07 - acc: 1.0000 - val_loss: 1.7927e-26 - val_acc: 1.0000\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.1333e-06 - acc: 1.0000 - val_loss: 1.7163e-26 - val_acc: 1.0000\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3.4656e-05 - acc: 1.0000 - val_loss: 1.6434e-26 - val_acc: 1.0000\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5.3289e-07 - acc: 1.0000 - val_loss: 1.5800e-26 - val_acc: 1.0000\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 4.0574e-07 - acc: 1.0000 - val_loss: 1.5247e-26 - val_acc: 1.0000\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3.5454e-06 - acc: 1.0000 - val_loss: 1.4758e-26 - val_acc: 1.0000\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5.9351e-07 - acc: 1.0000 - val_loss: 1.4328e-26 - val_acc: 1.0000\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3.0782e-06 - acc: 1.0000 - val_loss: 1.3944e-26 - val_acc: 1.0000\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 9.8533e-06 - acc: 1.0000 - val_loss: 1.3590e-26 - val_acc: 1.0000\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2.8743e-05 - acc: 1.0000 - val_loss: 1.3241e-26 - val_acc: 1.0000\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 4.0124e-05 - acc: 1.0000 - val_loss: 1.2881e-26 - val_acc: 1.0000\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.7967e-07 - acc: 1.0000 - val_loss: 1.2565e-26 - val_acc: 1.0000\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5.2331e-07 - acc: 1.0000 - val_loss: 1.2284e-26 - val_acc: 1.0000\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2.7119e-05 - acc: 1.0000 - val_loss: 1.2000e-26 - val_acc: 1.0000\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2.6681e-06 - acc: 1.0000 - val_loss: 1.1744e-26 - val_acc: 1.0000\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 9.5043e-06 - acc: 1.0000 - val_loss: 1.1503e-26 - val_acc: 1.0000\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.2010e-06 - acc: 1.0000 - val_loss: 1.1288e-26 - val_acc: 1.0000\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.7495e-05 - acc: 1.0000 - val_loss: 1.1076e-26 - val_acc: 1.0000\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3.4279e-06 - acc: 1.0000 - val_loss: 1.0884e-26 - val_acc: 1.0000\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 5.3096e-06 - acc: 1.0000 - val_loss: 1.0705e-26 - val_acc: 1.0000\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3.0524e-06 - acc: 1.0000 - val_loss: 1.0541e-26 - val_acc: 1.0000\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 7.0720e-07 - acc: 1.0000 - val_loss: 1.0393e-26 - val_acc: 1.0000\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5.3250e-05 - acc: 1.0000 - val_loss: 1.0207e-26 - val_acc: 1.0000\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.4464e-05 - acc: 1.0000 - val_loss: 1.0025e-26 - val_acc: 1.0000\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2.1888e-05 - acc: 1.0000 - val_loss: 9.8425e-27 - val_acc: 1.0000\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.1489e-05 - acc: 1.0000 - val_loss: 9.6678e-27 - val_acc: 1.0000\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4.4356e-05 - acc: 1.0000 - val_loss: 9.4616e-27 - val_acc: 1.0000\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3.4947e-04 - acc: 1.0000 - val_loss: 8.9987e-27 - val_acc: 1.0000\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 6.3771e-07 - acc: 1.0000 - val_loss: 8.5991e-27 - val_acc: 1.0000\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4.6959e-05 - acc: 1.0000 - val_loss: 8.2062e-27 - val_acc: 1.0000\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4.6495e-07 - acc: 1.0000 - val_loss: 7.8663e-27 - val_acc: 1.0000\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.7464e-05 - acc: 1.0000 - val_loss: 7.5555e-27 - val_acc: 1.0000\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.6130e-05 - acc: 1.0000 - val_loss: 7.2695e-27 - val_acc: 1.0000\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 9.5954e-05 - acc: 1.0000 - val_loss: 6.9646e-27 - val_acc: 1.0000\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 7.2752e-05 - acc: 1.0000 - val_loss: 6.6543e-27 - val_acc: 1.0000\n",
      "Epoch 178/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step - loss: 2.6180e-05 - acc: 1.0000 - val_loss: 6.3653e-27 - val_acc: 1.0000\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2.7027e-05 - acc: 1.0000 - val_loss: 6.0993e-27 - val_acc: 1.0000\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.6192e-04 - acc: 1.0000 - val_loss: 5.7573e-27 - val_acc: 1.0000\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.6072e-06 - acc: 1.0000 - val_loss: 5.4636e-27 - val_acc: 1.0000\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4.2633e-07 - acc: 1.0000 - val_loss: 5.2107e-27 - val_acc: 1.0000\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.7646e-06 - acc: 1.0000 - val_loss: 4.9912e-27 - val_acc: 1.0000\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 6.1316e-06 - acc: 1.0000 - val_loss: 4.7970e-27 - val_acc: 1.0000\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2.0482e-05 - acc: 1.0000 - val_loss: 4.6134e-27 - val_acc: 1.0000\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.7312e-06 - acc: 1.0000 - val_loss: 4.4524e-27 - val_acc: 1.0000\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.3666e-04 - acc: 1.0000 - val_loss: 4.2548e-27 - val_acc: 1.0000\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 8.0800e-07 - acc: 1.0000 - val_loss: 4.0834e-27 - val_acc: 1.0000\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.0945e-04 - acc: 1.0000 - val_loss: 3.8837e-27 - val_acc: 1.0000\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 2.9147e-05 - acc: 1.0000 - val_loss: 3.5177e-27 - val_acc: 1.0000\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 8.4953e-07 - acc: 1.0000 - val_loss: 3.3634e-27 - val_acc: 1.0000\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 9.3891e-07 - acc: 1.0000 - val_loss: 3.2293e-27 - val_acc: 1.0000\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4.2618e-06 - acc: 1.0000 - val_loss: 3.1109e-27 - val_acc: 1.0000\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 8.2714e-05 - acc: 1.0000 - val_loss: 2.9815e-27 - val_acc: 1.0000\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5.6854e-04 - acc: 1.0000 - val_loss: 2.7314e-27 - val_acc: 1.0000\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 6.2949e-06 - acc: 1.0000 - val_loss: 2.5210e-27 - val_acc: 1.0000\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3.6625e-06 - acc: 1.0000 - val_loss: 2.3436e-27 - val_acc: 1.0000\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.3081e-06 - acc: 1.0000 - val_loss: 2.1936e-27 - val_acc: 1.0000\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.7723e-06 - acc: 1.0000 - val_loss: 2.0656e-27 - val_acc: 1.0000\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 9.0252e-07 - acc: 1.0000 - val_loss: 1.9696e-27 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "#Defining the DGCNN model\n",
    "\n",
    "k = 35  # the number of rows for the output tensor\n",
    "layer_sizes = [32, 32, 32, 1]\n",
    "\n",
    "dgcnn_model = DeepGraphCNN(\n",
    "    layer_sizes=layer_sizes,\n",
    "    activations=[\"tanh\", \"tanh\", \"tanh\", \"tanh\"],\n",
    "    k=k,\n",
    "    bias=False,\n",
    "    generator=generator,\n",
    ")\n",
    "x_inp, x_out = dgcnn_model.in_out_tensors()\n",
    "\n",
    "x_out = Conv1D(filters=16, kernel_size=sum(layer_sizes), strides=sum(layer_sizes))(x_out)\n",
    "x_out = MaxPool1D(pool_size=2)(x_out)\n",
    "\n",
    "x_out = Conv1D(filters=32, kernel_size=5, strides=1)(x_out)\n",
    "\n",
    "x_out = Flatten()(x_out)\n",
    "\n",
    "x_out = Dense(units=128, activation=\"relu\")(x_out)\n",
    "x_out = Dropout(rate=0.5)(x_out)\n",
    "\n",
    "predictions = Dense(units=1, activation=\"sigmoid\")(x_out)\n",
    "\n",
    "model_2 = Model(inputs=x_inp, outputs=predictions)\n",
    "\n",
    "model_2.compile(\n",
    "    optimizer=Adam(lr=0.0001), loss=binary_crossentropy, metrics=[\"acc\"],\n",
    ")\n",
    "\n",
    "#Split the data\n",
    "train_graphs, test_graphs = model_selection.train_test_split(\n",
    "    dtrain_labels, train_size=0.9, test_size=None, stratify=dtrain_labels,\n",
    ")\n",
    "\n",
    "#Graph generators for split data\n",
    "gen = PaddedGraphGenerator(graphs=dtrain_graphs)\n",
    "\n",
    "train_gen = gen.flow(\n",
    "    list(train_graphs.index - 1),\n",
    "    targets=train_graphs.values,\n",
    "    batch_size=50,\n",
    "    symmetric_normalization=False,\n",
    ")\n",
    "\n",
    "test_gen = gen.flow(\n",
    "    list(test_graphs.index - 1),\n",
    "    targets=test_graphs.values,\n",
    "    batch_size=1,\n",
    "    symmetric_normalization=False,\n",
    ")\n",
    "\n",
    "#training the model\n",
    "history = model_2.fit(\n",
    "    train_gen, epochs=epochs, verbose=1, validation_data=test_gen, shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fb37d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step - loss: 1.9696e-27 - acc: 1.0000\n",
      "\n",
      "Test Set Metrics:\n",
      "\tloss: 0.0000\n",
      "\tacc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "test_metrics = model_2.evaluate(test_gen)\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "for name, val in zip(model_2.metrics_names, test_metrics):\n",
    "    print(\"\\t{}: {:0.4f}\".format(name, val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcd854c",
   "metadata": {},
   "source": [
    "# Evaluating both models on secluded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6d9807",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### GCN\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4635fd01",
   "metadata": {},
   "source": [
    "# Generating New Training Data using these models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2785c916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4/4 [==============================] - 0s 822us/step\n"
     ]
    }
   ],
   "source": [
    "#Choose which model to generate data with\n",
    "#model_choice = model_2\n",
    "model_choice = model_1\n",
    "\n",
    "#generate new labels for unlabeled graphs with model_choice \n",
    "new_gs,new_lbls = dataCreate.generateNewData(model_choice, unlabeled_graphs)\n",
    "\n",
    "#append newly generated data with existing data to create updated dataset\n",
    "updated_graphs, updated_labels = dataCreate.appendTwoDatasets(dtrain_graphs, dtrain_labels,new_gs,new_lbls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069c2b99",
   "metadata": {},
   "source": [
    "# Training on the New Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ddeac4",
   "metadata": {},
   "source": [
    "### Retraining GCN Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a2b9228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluating on fold 1 out of 10...\n",
      "Training and evaluating on fold 2 out of 10...\n",
      "Training and evaluating on fold 3 out of 10...\n",
      "Training and evaluating on fold 4 out of 10...\n",
      "Training and evaluating on fold 5 out of 10...\n",
      "Training and evaluating on fold 6 out of 10...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 22\u001b[0m\n\u001b[1;32m     16\u001b[0m train_gen, test_gen \u001b[38;5;241m=\u001b[39m get_generators(\n\u001b[1;32m     17\u001b[0m     train_index, test_index, updated_labels, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     20\u001b[0m model_1 \u001b[38;5;241m=\u001b[39m create_graph_classification_model(generator)\n\u001b[0;32m---> 22\u001b[0m history, acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_fold\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m test_accs\u001b[38;5;241m.\u001b[39mappend(acc)\n",
      "Cell \u001b[0;32mIn [4], line 20\u001b[0m, in \u001b[0;36mtrain_fold\u001b[0;34m(model, train_gen, test_gen, es, epochs)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_fold\u001b[39m(model, train_gen, test_gen, es, epochs):\n\u001b[0;32m---> 20\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mes\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# calculate performance on the test data and return along with history\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     test_metrics \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(test_gen, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/keras/engine/training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1642\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1643\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1644\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1648\u001b[0m ):\n\u001b[1;32m   1649\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1650\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1651\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1652\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    909\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    910\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    911\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 912\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    915\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    916\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m   (concrete_function,\n\u001b[1;32m    133\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1741\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1744\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1746\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1747\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m     args,\n\u001b[1;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1750\u001b[0m     executing_eagerly)\n\u001b[1;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    377\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    387\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    391\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "generator = PaddedGraphGenerator(graphs=updated_graphs)\n",
    "\n",
    "es = EarlyStopping(\n",
    "    monitor=\"val_loss\", min_delta=0, patience=25, restore_best_weights=True\n",
    ")\n",
    "\n",
    "test_accs = []\n",
    "\n",
    "#Create 10 fold training\n",
    "stratified_folds = model_selection.RepeatedStratifiedKFold(\n",
    "    n_splits=folds, n_repeats=n_repeats\n",
    ").split(updated_labels, updated_labels)\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(stratified_folds):\n",
    "    print(f\"Training and evaluating on fold {i+1} out of {folds * n_repeats}...\")\n",
    "    train_gen, test_gen = get_generators(\n",
    "        train_index, test_index, updated_labels, batch_size=30\n",
    "    )\n",
    "\n",
    "    model_1 = create_graph_classification_model(generator)\n",
    "\n",
    "    history, acc = train_fold(model_1, train_gen, test_gen, es, epochs)\n",
    "\n",
    "    test_accs.append(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28ce4e4",
   "metadata": {},
   "source": [
    "### Retraining DCGNN Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac3b14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data\n",
    "train_graphs, test_graphs = model_selection.train_test_split(\n",
    "    dtrain_labels, train_size=0.9, test_size=None, stratify=updated_labels,\n",
    ")\n",
    "\n",
    "#Graph generators for split data\n",
    "gen = PaddedGraphGenerator(graphs=updated_graphs)\n",
    "\n",
    "train_gen = gen.flow(\n",
    "    list(train_graphs.index - 1),\n",
    "    targets=train_graphs.values,\n",
    "    batch_size=50,\n",
    "    symmetric_normalization=False,\n",
    ")\n",
    "\n",
    "test_gen = gen.flow(\n",
    "    list(test_graphs.index - 1),\n",
    "    targets=test_graphs.values,\n",
    "    batch_size=1,\n",
    "    symmetric_normalization=False,\n",
    ")\n",
    "\n",
    "#training the model\n",
    "history = model_2.fit(\n",
    "    train_gen, epochs=epochs, verbose=1, validation_data=test_gen, shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4638757",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
