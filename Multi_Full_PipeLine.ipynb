{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b522ce5a",
   "metadata": {},
   "source": [
    "# Flow:\n",
    "\n",
    "1) Read Data from folder\n",
    "2) Create Graphs & Labels and seperate into test, train & validation (Set1A, Set1B, Set 1C)\n",
    "4) Train GCN & DCGNN Models on the same (Model1 & Model2)\n",
    "5) Evaluate the model on Set1A \n",
    "6) Obtain Unlabelled graphs\n",
    "7) Create labels for these unlabelled graphs using trained model (Set2)\n",
    "8) Combine these trainable datasets as one complete SetC (Set1A, Set1B & Set 2)\n",
    "9) Train a new model1 with this combined data SetC\n",
    "10) Evaluate the model1 (trained new) on Set1A to check if performance has improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59bc4743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import stellargraph as sg\n",
    "from stellargraph.mapper import PaddedGraphGenerator\n",
    "from stellargraph.layer import GCNSupervisedGraphClassification\n",
    "from stellargraph.layer import DeepGraphCNN\n",
    "from stellargraph import StellarGraph\n",
    "\n",
    "from stellargraph import datasets\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Conv1D, MaxPool1D, Dropout, Flatten\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow.keras.utils as ku\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import createGraph\n",
    "import dataCreate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfffb9c",
   "metadata": {},
   "source": [
    "# Step 1) &  Step 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c144f587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "train_folder = 'graphsmallds/'\n",
    "dtrain_graphs,dtrain_labels = createGraph.readLabeledFolder(train_folder)\n",
    "\n",
    "print(len(dtrain_graphs))\n",
    "print(len(dtrain_labels))\n",
    "\n",
    "#val_folder = ''\n",
    "#val_graphs,val_labels = createGraph.readLabeledFolder(train_folder)\n",
    "\n",
    "#unlabeled_folder = ''\n",
    "#unlabeled_graphs = createGrapg.readUnlabeledFolder(unlabeled_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "eddf0460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     1\n",
      "1     2\n",
      "2     3\n",
      "3     4\n",
      "4     0\n",
      "5     1\n",
      "6     2\n",
      "7     3\n",
      "8     4\n",
      "9     0\n",
      "10    1\n",
      "11    2\n",
      "12    3\n",
      "13    4\n",
      "14    0\n",
      "dtype: category\n",
      "Categories (5, int64): [0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "print(dtrain_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d558a9",
   "metadata": {},
   "source": [
    "# Splitting & Training Data Step 3) & 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5513b4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper Parameters \n",
    "epochs = 200  # maximum number of training epochs\n",
    "folds = 2 # the number of folds for k-fold cross validation\n",
    "n_repeats = 5  # the number of repeats for repeated k-fold cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972956c8",
   "metadata": {},
   "source": [
    "### GCN Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dfc32b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluating on fold 1 out of 10...\n",
      "Training and evaluating on fold 2 out of 10...\n"
     ]
    }
   ],
   "source": [
    "def create_graph_classification_model(generator):\n",
    "    gc_model = GCNSupervisedGraphClassification(\n",
    "        layer_sizes=[64, 64],\n",
    "        activations=[\"relu\", \"relu\"],\n",
    "        generator=generator,\n",
    "        dropout=0.5,\n",
    "    )\n",
    "    x_inp, x_out = gc_model.in_out_tensors()\n",
    "    predictions = Dense(units=32, activation=\"relu\")(x_out)\n",
    "    predictions = Dense(units=16, activation=\"relu\")(predictions)\n",
    "    predictions = Dense(units=5, activation=\"softmax\")(predictions)\n",
    "\n",
    "    # Let's create the Keras model and prepare it for training\n",
    "    model = Model(inputs=x_inp, outputs=predictions)\n",
    "    model.compile(optimizer=Adam(0.005), loss=tf.keras.losses.CategoricalCrossentropy(), metrics=[\"acc\"])\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_fold(model, train_gen, test_gen, es, epochs):\n",
    "    history = model.fit(\n",
    "        train_gen, epochs=epochs, validation_data=test_gen, verbose=0, callbacks=[es],\n",
    "    )\n",
    "    # calculate performance on the test data and return along with history\n",
    "    test_metrics = model.evaluate(test_gen, verbose=0)\n",
    "    test_acc = test_metrics[model.metrics_names.index(\"acc\")]\n",
    "\n",
    "    return history, test_acc\n",
    "\n",
    "def get_generators(train_index, test_index, graph_labels, batch_size):\n",
    "    train_gen = generator.flow(\n",
    "        train_index, targets=graph_labels[train_index], batch_size=batch_size\n",
    "    )\n",
    "    test_gen = generator.flow(\n",
    "        test_index, targets=graph_labels[test_index], batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    return train_gen, test_gen\n",
    "\n",
    "# Convert the dtrain_labels to a numpy array\n",
    "labels = dtrain_labels.cat.codes.values\n",
    "\n",
    "# Convert the labels to one-hot encoded vectors\n",
    "dtrain_labels_gcn = ku.to_categorical(labels)\n",
    "\n",
    "print(dtrain_labels_gcn)\n",
    "\n",
    "generator = PaddedGraphGenerator(graphs=dtrain_graphs)\n",
    "\n",
    "es = EarlyStopping(\n",
    "    monitor=\"val_loss\", min_delta=0, patience=25, restore_best_weights=True\n",
    ")\n",
    "\n",
    "test_accs = []\n",
    "\n",
    "#Create 10 fold training\n",
    "stratified_folds = KFold(\n",
    "    n_splits=folds\n",
    ").split(dtrain_labels, dtrain_labels_gcn)\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(stratified_folds):\n",
    "    print(f\"Training and evaluating on fold {i+1} out of {folds * n_repeats}...\")\n",
    "    train_gen, test_gen = get_generators(\n",
    "        train_index, test_index, dtrain_labels_gcn, batch_size=30\n",
    "    )\n",
    "\n",
    "    model_1 = create_graph_classification_model(generator)\n",
    "\n",
    "    history, acc = train_fold(model_1, train_gen, test_gen, es, epochs)\n",
    "\n",
    "    test_accs.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "78066473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy over all folds mean: 79.5% and std: 8.0%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Count')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\n",
    "    f\"Accuracy over all folds mean: {np.mean(test_accs)*100:.3}% and std: {np.std(test_accs)*100:.2}%\"\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(test_accs)\n",
    "plt.xlabel(\"Accuracy\")\n",
    "plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfe4e12",
   "metadata": {},
   "source": [
    "### DGCNN Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "195d495b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0    1    2    3    4\n",
      "0   0.0  1.0  0.0  0.0  0.0\n",
      "1   0.0  0.0  1.0  0.0  0.0\n",
      "2   0.0  0.0  0.0  1.0  0.0\n",
      "3   0.0  0.0  0.0  0.0  1.0\n",
      "4   1.0  0.0  0.0  0.0  0.0\n",
      "5   0.0  1.0  0.0  0.0  0.0\n",
      "6   0.0  0.0  1.0  0.0  0.0\n",
      "7   0.0  0.0  0.0  1.0  0.0\n",
      "8   0.0  0.0  0.0  0.0  1.0\n",
      "9   1.0  0.0  0.0  0.0  0.0\n",
      "10  0.0  1.0  0.0  0.0  0.0\n",
      "11  0.0  0.0  1.0  0.0  0.0\n",
      "12  0.0  0.0  0.0  1.0  0.0\n",
      "13  0.0  0.0  0.0  0.0  1.0\n",
      "14  1.0  0.0  0.0  0.0  0.0\n",
      "(10, 5)\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 1s 742ms/step - loss: 0.6983 - acc: 0.1000 - val_loss: 0.6821 - val_acc: 0.4000\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6956 - acc: 0.2000 - val_loss: 0.6578 - val_acc: 0.6000\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6601 - acc: 0.5000 - val_loss: 0.6369 - val_acc: 0.6000\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.6428 - acc: 0.2000 - val_loss: 0.6128 - val_acc: 0.6000\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.6189 - acc: 0.3000 - val_loss: 0.5846 - val_acc: 0.6000\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5748 - acc: 0.7000 - val_loss: 0.5524 - val_acc: 0.6000\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.5693 - acc: 0.4000 - val_loss: 0.5196 - val_acc: 0.6000\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.5331 - acc: 0.5000 - val_loss: 0.4859 - val_acc: 0.8000\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.5157 - acc: 0.4000 - val_loss: 0.4538 - val_acc: 0.8000\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.4847 - acc: 0.4000 - val_loss: 0.4230 - val_acc: 0.8000\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.4300 - acc: 0.7000 - val_loss: 0.3947 - val_acc: 0.8000\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.4127 - acc: 0.6000 - val_loss: 0.3692 - val_acc: 0.8000\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3870 - acc: 0.6000 - val_loss: 0.3463 - val_acc: 0.8000\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3862 - acc: 0.6000 - val_loss: 0.3245 - val_acc: 0.8000\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3724 - acc: 0.4000 - val_loss: 0.3029 - val_acc: 0.8000\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3661 - acc: 0.6000 - val_loss: 0.2837 - val_acc: 0.8000\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2924 - acc: 0.9000 - val_loss: 0.2635 - val_acc: 0.8000\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.2929 - acc: 0.6000 - val_loss: 0.2442 - val_acc: 0.8000\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.3234 - acc: 0.6000 - val_loss: 0.2272 - val_acc: 0.8000\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.3209 - acc: 0.7000 - val_loss: 0.2139 - val_acc: 0.8000\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2254 - acc: 0.8000 - val_loss: 0.2035 - val_acc: 0.8000\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2807 - acc: 0.9000 - val_loss: 0.1930 - val_acc: 0.8000\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2727 - acc: 0.6000 - val_loss: 0.1840 - val_acc: 0.8000\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2707 - acc: 0.7000 - val_loss: 0.1738 - val_acc: 0.8000\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2904 - acc: 0.6000 - val_loss: 0.1651 - val_acc: 0.8000\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2591 - acc: 0.7000 - val_loss: 0.1582 - val_acc: 0.8000\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1767 - acc: 0.8000 - val_loss: 0.1517 - val_acc: 0.8000\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2100 - acc: 0.7000 - val_loss: 0.1478 - val_acc: 0.8000\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2343 - acc: 0.6000 - val_loss: 0.1453 - val_acc: 0.8000\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1765 - acc: 0.8000 - val_loss: 0.1415 - val_acc: 0.8000\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1936 - acc: 0.8000 - val_loss: 0.1391 - val_acc: 0.8000\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1591 - acc: 0.8000 - val_loss: 0.1391 - val_acc: 0.8000\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2409 - acc: 0.6000 - val_loss: 0.1369 - val_acc: 0.8000\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2344 - acc: 0.7000 - val_loss: 0.1341 - val_acc: 0.8000\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1974 - acc: 0.8000 - val_loss: 0.1313 - val_acc: 0.8000\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1670 - acc: 0.9000 - val_loss: 0.1289 - val_acc: 0.8000\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1641 - acc: 0.8000 - val_loss: 0.1274 - val_acc: 0.8000\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1378 - acc: 1.0000 - val_loss: 0.1261 - val_acc: 0.8000\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1678 - acc: 0.8000 - val_loss: 0.1253 - val_acc: 0.8000\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1685 - acc: 0.7000 - val_loss: 0.1245 - val_acc: 0.8000\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1243 - acc: 0.8000 - val_loss: 0.1235 - val_acc: 0.8000\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1875 - acc: 0.8000 - val_loss: 0.1226 - val_acc: 0.8000\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1321 - acc: 0.8000 - val_loss: 0.1217 - val_acc: 0.8000\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1311 - acc: 0.8000 - val_loss: 0.1206 - val_acc: 0.8000\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1544 - acc: 0.8000 - val_loss: 0.1198 - val_acc: 0.8000\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2160 - acc: 0.7000 - val_loss: 0.1192 - val_acc: 0.8000\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1692 - acc: 0.7000 - val_loss: 0.1191 - val_acc: 0.8000\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1354 - acc: 0.8000 - val_loss: 0.1191 - val_acc: 0.8000\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1382 - acc: 0.8000 - val_loss: 0.1193 - val_acc: 0.8000\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1487 - acc: 0.8000 - val_loss: 0.1193 - val_acc: 0.8000\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1803 - acc: 0.7000 - val_loss: 0.1194 - val_acc: 0.8000\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1842 - acc: 0.6000 - val_loss: 0.1196 - val_acc: 0.8000\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1175 - acc: 0.9000 - val_loss: 0.1193 - val_acc: 0.8000\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1469 - acc: 0.7000 - val_loss: 0.1189 - val_acc: 0.8000\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1720 - acc: 0.7000 - val_loss: 0.1185 - val_acc: 0.8000\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1241 - acc: 0.8000 - val_loss: 0.1180 - val_acc: 0.8000\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1612 - acc: 0.7000 - val_loss: 0.1170 - val_acc: 0.8000\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1382 - acc: 0.8000 - val_loss: 0.1161 - val_acc: 0.8000\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1602 - acc: 0.7000 - val_loss: 0.1154 - val_acc: 0.8000\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1393 - acc: 0.9000 - val_loss: 0.1150 - val_acc: 0.8000\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1848 - acc: 0.7000 - val_loss: 0.1147 - val_acc: 0.8000\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1572 - acc: 0.7000 - val_loss: 0.1147 - val_acc: 0.8000\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1351 - acc: 0.8000 - val_loss: 0.1148 - val_acc: 0.8000\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1627 - acc: 0.7000 - val_loss: 0.1150 - val_acc: 0.8000\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1826 - acc: 0.7000 - val_loss: 0.1156 - val_acc: 0.8000\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1132 - acc: 1.0000 - val_loss: 0.1161 - val_acc: 0.8000\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1738 - acc: 0.7000 - val_loss: 0.1168 - val_acc: 0.8000\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1361 - acc: 0.9000 - val_loss: 0.1172 - val_acc: 0.8000\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1570 - acc: 0.7000 - val_loss: 0.1174 - val_acc: 0.8000\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1396 - acc: 0.8000 - val_loss: 0.1174 - val_acc: 0.8000\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1477 - acc: 0.8000 - val_loss: 0.1175 - val_acc: 0.8000\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1502 - acc: 0.7000 - val_loss: 0.1174 - val_acc: 0.8000\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1040 - acc: 0.8000 - val_loss: 0.1171 - val_acc: 0.8000\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1278 - acc: 0.8000 - val_loss: 0.1169 - val_acc: 0.8000\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1171 - acc: 1.0000 - val_loss: 0.1167 - val_acc: 0.8000\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1526 - acc: 0.8000 - val_loss: 0.1164 - val_acc: 0.8000\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1356 - acc: 0.8000 - val_loss: 0.1162 - val_acc: 0.8000\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0936 - acc: 0.9000 - val_loss: 0.1159 - val_acc: 0.8000\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1675 - acc: 0.8000 - val_loss: 0.1156 - val_acc: 0.8000\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1162 - acc: 0.9000 - val_loss: 0.1152 - val_acc: 0.8000\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1308 - acc: 0.6000 - val_loss: 0.1148 - val_acc: 0.8000\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1320 - acc: 0.8000 - val_loss: 0.1145 - val_acc: 0.8000\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1174 - acc: 0.9000 - val_loss: 0.1141 - val_acc: 0.8000\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1442 - acc: 0.7000 - val_loss: 0.1138 - val_acc: 0.8000\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1045 - acc: 0.9000 - val_loss: 0.1135 - val_acc: 0.8000\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1182 - acc: 0.8000 - val_loss: 0.1133 - val_acc: 0.8000\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1128 - acc: 0.8000 - val_loss: 0.1132 - val_acc: 0.8000\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1098 - acc: 0.9000 - val_loss: 0.1131 - val_acc: 0.8000\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1213 - acc: 0.8000 - val_loss: 0.1130 - val_acc: 0.8000\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1331 - acc: 0.8000 - val_loss: 0.1130 - val_acc: 0.8000\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1450 - acc: 0.7000 - val_loss: 0.1129 - val_acc: 0.8000\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1197 - acc: 0.8000 - val_loss: 0.1127 - val_acc: 0.8000\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1252 - acc: 0.9000 - val_loss: 0.1125 - val_acc: 0.8000\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1428 - acc: 0.8000 - val_loss: 0.1124 - val_acc: 0.8000\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1624 - acc: 0.8000 - val_loss: 0.1123 - val_acc: 0.8000\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1505 - acc: 0.9000 - val_loss: 0.1122 - val_acc: 0.8000\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1059 - acc: 0.9000 - val_loss: 0.1122 - val_acc: 0.8000\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.1293 - acc: 0.8000 - val_loss: 0.1123 - val_acc: 0.8000\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1168 - acc: 0.8000 - val_loss: 0.1125 - val_acc: 0.8000\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1222 - acc: 0.8000 - val_loss: 0.1128 - val_acc: 0.8000\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1427 - acc: 0.7000 - val_loss: 0.1129 - val_acc: 0.8000\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1107 - acc: 0.8000 - val_loss: 0.1130 - val_acc: 0.8000\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1408 - acc: 0.9000 - val_loss: 0.1129 - val_acc: 0.8000\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1317 - acc: 0.9000 - val_loss: 0.1128 - val_acc: 0.8000\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1033 - acc: 1.0000 - val_loss: 0.1126 - val_acc: 0.8000\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1506 - acc: 0.8000 - val_loss: 0.1125 - val_acc: 0.8000\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1527 - acc: 0.7000 - val_loss: 0.1126 - val_acc: 0.8000\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1503 - acc: 0.7000 - val_loss: 0.1128 - val_acc: 0.8000\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1615 - acc: 0.8000 - val_loss: 0.1129 - val_acc: 0.8000\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0983 - acc: 1.0000 - val_loss: 0.1129 - val_acc: 0.8000\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1056 - acc: 0.9000 - val_loss: 0.1129 - val_acc: 0.8000\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1243 - acc: 0.9000 - val_loss: 0.1127 - val_acc: 0.8000\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1580 - acc: 0.7000 - val_loss: 0.1127 - val_acc: 0.8000\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0929 - acc: 0.9000 - val_loss: 0.1126 - val_acc: 0.8000\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1105 - acc: 0.8000 - val_loss: 0.1125 - val_acc: 0.8000\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1347 - acc: 0.8000 - val_loss: 0.1125 - val_acc: 0.8000\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1096 - acc: 0.9000 - val_loss: 0.1125 - val_acc: 0.8000\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1524 - acc: 0.8000 - val_loss: 0.1124 - val_acc: 0.8000\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1469 - acc: 0.7000 - val_loss: 0.1125 - val_acc: 0.8000\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1563 - acc: 0.7000 - val_loss: 0.1125 - val_acc: 0.8000\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0904 - acc: 1.0000 - val_loss: 0.1125 - val_acc: 0.8000\n",
      "Epoch 122/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1142 - acc: 0.9000 - val_loss: 0.1126 - val_acc: 0.8000\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1217 - acc: 0.7000 - val_loss: 0.1126 - val_acc: 0.8000\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1322 - acc: 0.8000 - val_loss: 0.1126 - val_acc: 0.8000\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1010 - acc: 1.0000 - val_loss: 0.1126 - val_acc: 0.8000\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1329 - acc: 0.8000 - val_loss: 0.1126 - val_acc: 0.8000\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1211 - acc: 0.9000 - val_loss: 0.1125 - val_acc: 0.8000\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1462 - acc: 0.6000 - val_loss: 0.1125 - val_acc: 0.8000\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1388 - acc: 0.7000 - val_loss: 0.1126 - val_acc: 0.8000\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1280 - acc: 0.8000 - val_loss: 0.1126 - val_acc: 0.8000\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1356 - acc: 0.8000 - val_loss: 0.1126 - val_acc: 0.8000\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.1267 - acc: 0.8000 - val_loss: 0.1127 - val_acc: 0.8000\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.1810 - acc: 0.7000 - val_loss: 0.1127 - val_acc: 0.8000\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.1530 - acc: 0.7000 - val_loss: 0.1128 - val_acc: 0.8000\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.1190 - acc: 0.8000 - val_loss: 0.1129 - val_acc: 0.8000\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1903 - acc: 0.7000 - val_loss: 0.1131 - val_acc: 0.8000\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1098 - acc: 0.8000 - val_loss: 0.1133 - val_acc: 0.8000\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1515 - acc: 0.6000 - val_loss: 0.1134 - val_acc: 0.8000\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1348 - acc: 0.7000 - val_loss: 0.1135 - val_acc: 0.8000\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1253 - acc: 0.8000 - val_loss: 0.1135 - val_acc: 0.8000\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1130 - acc: 0.7000 - val_loss: 0.1136 - val_acc: 0.8000\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1581 - acc: 0.7000 - val_loss: 0.1136 - val_acc: 0.8000\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1351 - acc: 0.8000 - val_loss: 0.1136 - val_acc: 0.8000\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1363 - acc: 0.8000 - val_loss: 0.1137 - val_acc: 0.8000\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0940 - acc: 0.8000 - val_loss: 0.1137 - val_acc: 0.8000\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1503 - acc: 0.6000 - val_loss: 0.1137 - val_acc: 0.8000\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1460 - acc: 0.7000 - val_loss: 0.1138 - val_acc: 0.8000\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.1219 - acc: 0.9000 - val_loss: 0.1139 - val_acc: 0.8000\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.1300 - acc: 0.8000 - val_loss: 0.1139 - val_acc: 0.8000\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.1234 - acc: 0.7000 - val_loss: 0.1139 - val_acc: 0.8000\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1525 - acc: 0.7000 - val_loss: 0.1140 - val_acc: 0.8000\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1238 - acc: 0.7000 - val_loss: 0.1141 - val_acc: 0.8000\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.1003 - acc: 0.9000 - val_loss: 0.1141 - val_acc: 0.8000\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1583 - acc: 0.6000 - val_loss: 0.1142 - val_acc: 0.8000\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1102 - acc: 0.9000 - val_loss: 0.1142 - val_acc: 0.8000\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1136 - acc: 0.9000 - val_loss: 0.1142 - val_acc: 0.8000\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0994 - acc: 1.0000 - val_loss: 0.1141 - val_acc: 0.8000\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1235 - acc: 0.8000 - val_loss: 0.1140 - val_acc: 0.8000\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.1425 - acc: 0.8000 - val_loss: 0.1139 - val_acc: 0.8000\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1333 - acc: 0.8000 - val_loss: 0.1139 - val_acc: 0.8000\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1531 - acc: 0.7000 - val_loss: 0.1139 - val_acc: 0.8000\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1178 - acc: 0.8000 - val_loss: 0.1139 - val_acc: 0.8000\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1174 - acc: 0.8000 - val_loss: 0.1139 - val_acc: 0.8000\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.1212 - acc: 0.8000 - val_loss: 0.1139 - val_acc: 0.8000\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1588 - acc: 0.6000 - val_loss: 0.1139 - val_acc: 0.8000\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1359 - acc: 0.8000 - val_loss: 0.1139 - val_acc: 0.8000\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1169 - acc: 0.7000 - val_loss: 0.1139 - val_acc: 0.8000\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1033 - acc: 1.0000 - val_loss: 0.1138 - val_acc: 0.8000\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0963 - acc: 0.9000 - val_loss: 0.1136 - val_acc: 0.8000\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.1449 - acc: 0.8000 - val_loss: 0.1134 - val_acc: 0.8000\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1317 - acc: 0.8000 - val_loss: 0.1132 - val_acc: 0.8000\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1361 - acc: 0.7000 - val_loss: 0.1131 - val_acc: 0.8000\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1676 - acc: 0.6000 - val_loss: 0.1131 - val_acc: 0.8000\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1382 - acc: 0.7000 - val_loss: 0.1130 - val_acc: 0.8000\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1325 - acc: 0.8000 - val_loss: 0.1130 - val_acc: 0.8000\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1082 - acc: 0.9000 - val_loss: 0.1129 - val_acc: 0.8000\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1174 - acc: 0.8000 - val_loss: 0.1128 - val_acc: 0.8000\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1280 - acc: 0.8000 - val_loss: 0.1128 - val_acc: 0.8000\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1134 - acc: 0.9000 - val_loss: 0.1127 - val_acc: 0.8000\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1362 - acc: 0.8000 - val_loss: 0.1127 - val_acc: 0.8000\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1315 - acc: 0.7000 - val_loss: 0.1126 - val_acc: 0.8000\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.1375 - acc: 0.8000 - val_loss: 0.1127 - val_acc: 0.8000\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1164 - acc: 0.8000 - val_loss: 0.1127 - val_acc: 0.8000\n",
      "Epoch 184/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0874 - acc: 1.0000 - val_loss: 0.1126 - val_acc: 0.8000\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1063 - acc: 0.9000 - val_loss: 0.1126 - val_acc: 0.8000\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1259 - acc: 0.8000 - val_loss: 0.1125 - val_acc: 0.8000\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1137 - acc: 0.8000 - val_loss: 0.1124 - val_acc: 0.8000\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1443 - acc: 0.6000 - val_loss: 0.1124 - val_acc: 0.8000\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0952 - acc: 1.0000 - val_loss: 0.1124 - val_acc: 0.8000\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1038 - acc: 1.0000 - val_loss: 0.1123 - val_acc: 0.8000\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1112 - acc: 1.0000 - val_loss: 0.1122 - val_acc: 0.8000\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1379 - acc: 0.8000 - val_loss: 0.1121 - val_acc: 0.8000\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1168 - acc: 0.7000 - val_loss: 0.1120 - val_acc: 0.8000\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1287 - acc: 0.8000 - val_loss: 0.1120 - val_acc: 0.8000\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1251 - acc: 0.7000 - val_loss: 0.1120 - val_acc: 0.8000\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1297 - acc: 0.7000 - val_loss: 0.1120 - val_acc: 0.8000\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1229 - acc: 0.8000 - val_loss: 0.1120 - val_acc: 0.8000\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1017 - acc: 0.8000 - val_loss: 0.1120 - val_acc: 0.8000\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1624 - acc: 0.7000 - val_loss: 0.1120 - val_acc: 0.8000\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0982 - acc: 0.8000 - val_loss: 0.1120 - val_acc: 0.8000\n"
     ]
    }
   ],
   "source": [
    "#Defining the DGCNN model\n",
    "\n",
    "k = 35  # the number of rows for the output tensor\n",
    "layer_sizes = [32, 32, 32, 1]\n",
    "\n",
    "dgcnn_model = DeepGraphCNN(\n",
    "    layer_sizes=layer_sizes,\n",
    "    activations=[\"tanh\", \"tanh\", \"tanh\", \"tanh\"],\n",
    "    k=k,\n",
    "    bias=False,\n",
    "    generator=generator,\n",
    ")\n",
    "x_inp, x_out = dgcnn_model.in_out_tensors()\n",
    "\n",
    "x_out = Conv1D(filters=16, kernel_size=sum(layer_sizes), strides=sum(layer_sizes))(x_out)\n",
    "x_out = MaxPool1D(pool_size=2)(x_out)\n",
    "\n",
    "x_out = Conv1D(filters=32, kernel_size=5, strides=1)(x_out)\n",
    "\n",
    "x_out = Flatten()(x_out)\n",
    "\n",
    "x_out = Dense(units=128, activation=\"relu\")(x_out)\n",
    "x_out = Dropout(rate=0.5)(x_out)\n",
    "\n",
    "predictions = Dense(units=5, activation=\"softmax\")(x_out)\n",
    "\n",
    "model_2 = Model(inputs=x_inp, outputs=predictions)\n",
    "\n",
    "model_2.compile(\n",
    "    optimizer=Adam(lr=0.0001), loss=binary_crossentropy, metrics=[\"acc\"],\n",
    ")\n",
    "\n",
    "df = pd.DataFrame(dtrain_labels_gcn)\n",
    "print(df)\n",
    "\n",
    "#Split the data\n",
    "train_graphs, test_graphs = model_selection.train_test_split(\n",
    "    df, train_size=10, test_size=5, stratify=dtrain_labels,)\n",
    "\n",
    "print(train_graphs.shape)\n",
    "\n",
    "#Graph generators for split data\n",
    "gen = PaddedGraphGenerator(graphs=dtrain_graphs)\n",
    "\n",
    "train_gen = gen.flow(\n",
    "    list(train_graphs.index - 1),\n",
    "    targets=train_graphs.values,\n",
    "    batch_size=50,\n",
    "    symmetric_normalization=False,\n",
    ")\n",
    "\n",
    "test_gen = gen.flow(\n",
    "    list(test_graphs.index - 1),\n",
    "    targets=test_graphs.values,\n",
    "    batch_size=1,\n",
    "    symmetric_normalization=False,\n",
    ")\n",
    "#training the model\n",
    "history = model_2.fit(\n",
    "    train_gen, epochs=epochs, verbose=1, validation_data=test_gen, shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2fb37d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 1ms/step - loss: 0.1120 - acc: 0.8000\n",
      "\n",
      "Test Set Metrics:\n",
      "\tloss: 0.1120\n",
      "\tacc: 0.8000\n"
     ]
    }
   ],
   "source": [
    "test_metrics = model_2.evaluate(test_gen)\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "for name, val in zip(model_2.metrics_names, test_metrics):\n",
    "    print(\"\\t{}: {:0.4f}\".format(name, val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcd854c",
   "metadata": {},
   "source": [
    "# Evaluating both models on secluded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2a6d9807",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### GCN\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4635fd01",
   "metadata": {},
   "source": [
    "# Generating New Training Data using these models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2785c916",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unlabeled_graphs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [70], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m model_choice \u001b[38;5;241m=\u001b[39m model_1\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#generate new labels for unlabeled graphs with model_choice \u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m new_gs,new_lbls \u001b[38;5;241m=\u001b[39m dataCreate\u001b[38;5;241m.\u001b[39mgenerateNewData(model_choice, \u001b[43munlabeled_graphs\u001b[49m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#append newly generated data with existing data to create updated dataset\u001b[39;00m\n\u001b[1;32m      9\u001b[0m updated_graphs, updated_labels \u001b[38;5;241m=\u001b[39m dataCreate\u001b[38;5;241m.\u001b[39mappendTwoDatasets(dtrain_graphs, dtrain_labels,new_gs,new_lbls)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'unlabeled_graphs' is not defined"
     ]
    }
   ],
   "source": [
    "#Choose which model to generate data with\n",
    "#model_choice = model_2\n",
    "model_choice = model_1\n",
    "\n",
    "#generate new labels for unlabeled graphs with model_choice \n",
    "new_gs,new_lbls = dataCreate.generateNewData(model_choice, unlabeled_graphs)\n",
    "\n",
    "#append newly generated data with existing data to create updated dataset\n",
    "updated_graphs, updated_labels = dataCreate.appendTwoDatasets(dtrain_graphs, dtrain_labels,new_gs,new_lbls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb337614",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOR REUSING CODE FROM ABOVE, WE RENAME THE NEW DATA OBJECTS WITH THE ORIGINAL NAME\n",
    "\n",
    "\n",
    "\n",
    "#Originally the data sizes are:\n",
    "\n",
    "print(len(dtrain_graphs))\n",
    "print(len(dtrain_labels))\n",
    "\n",
    "\n",
    "#We use the old names to refer to the new data now\n",
    "dtrain_graphs = updated_graphs\n",
    "dtrain_labels = updated_labels\n",
    "\n",
    "\n",
    "#Data of new objects now (Should be different)\n",
    "print(len(dtrain_graphs))\n",
    "print(len(dtrain_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069c2b99",
   "metadata": {},
   "source": [
    "# Training on the New Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ddeac4",
   "metadata": {},
   "source": [
    "### Retraining GCN Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a2b9228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluating on fold 1 out of 10...\n",
      "Training and evaluating on fold 2 out of 10...\n",
      "Training and evaluating on fold 3 out of 10...\n",
      "Training and evaluating on fold 4 out of 10...\n",
      "Training and evaluating on fold 5 out of 10...\n",
      "Training and evaluating on fold 6 out of 10...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 22\u001b[0m\n\u001b[1;32m     16\u001b[0m train_gen, test_gen \u001b[38;5;241m=\u001b[39m get_generators(\n\u001b[1;32m     17\u001b[0m     train_index, test_index, updated_labels, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     20\u001b[0m model_1 \u001b[38;5;241m=\u001b[39m create_graph_classification_model(generator)\n\u001b[0;32m---> 22\u001b[0m history, acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_fold\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m test_accs\u001b[38;5;241m.\u001b[39mappend(acc)\n",
      "Cell \u001b[0;32mIn [4], line 20\u001b[0m, in \u001b[0;36mtrain_fold\u001b[0;34m(model, train_gen, test_gen, es, epochs)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_fold\u001b[39m(model, train_gen, test_gen, es, epochs):\n\u001b[0;32m---> 20\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mes\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# calculate performance on the test data and return along with history\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     test_metrics \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(test_gen, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/keras/engine/training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1642\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1643\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1644\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1648\u001b[0m ):\n\u001b[1;32m   1649\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1650\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1651\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1652\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    909\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    910\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    911\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 912\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    915\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    916\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m   (concrete_function,\n\u001b[1;32m    133\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1741\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1744\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1746\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1747\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m     args,\n\u001b[1;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1750\u001b[0m     executing_eagerly)\n\u001b[1;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    377\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    387\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    391\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Convert the dtrain_labels to a numpy array\n",
    "labels = dtrain_labels.cat.codes.values\n",
    "\n",
    "# Convert the labels to one-hot encoded vectors\n",
    "dtrain_labels_gcn = ku.to_categorical(labels)\n",
    "\n",
    "print(dtrain_labels_gcn)\n",
    "\n",
    "generator = PaddedGraphGenerator(graphs=dtrain_graphs)\n",
    "\n",
    "es = EarlyStopping(\n",
    "    monitor=\"val_loss\", min_delta=0, patience=25, restore_best_weights=True\n",
    ")\n",
    "\n",
    "test_accs = []\n",
    "\n",
    "#Create 10 fold training\n",
    "stratified_folds = KFold(\n",
    "    n_splits=folds\n",
    ").split(dtrain_labels, dtrain_labels_gcn)\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(stratified_folds):\n",
    "    print(f\"Training and evaluating on fold {i+1} out of {folds * n_repeats}...\")\n",
    "    train_gen, test_gen = get_generators(\n",
    "        train_index, test_index, dtrain_labels_gcn, batch_size=30\n",
    "    )\n",
    "\n",
    "    model_1 = create_graph_classification_model(generator)\n",
    "\n",
    "    history, acc = train_fold(model_1, train_gen, test_gen, es, epochs)\n",
    "\n",
    "    test_accs.append(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28ce4e4",
   "metadata": {},
   "source": [
    "### Retraining DCGNN Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac3b14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dtrain_labels_gcn)\n",
    "print(df)\n",
    "\n",
    "#Split the data\n",
    "train_graphs, test_graphs = model_selection.train_test_split(\n",
    "    df, train_size=10, test_size=5, stratify=dtrain_labels,)\n",
    "\n",
    "print(train_graphs.shape)\n",
    "\n",
    "#Graph generators for split data\n",
    "gen = PaddedGraphGenerator(graphs=dtrain_graphs)\n",
    "\n",
    "train_gen = gen.flow(\n",
    "    list(train_graphs.index - 1),\n",
    "    targets=train_graphs.values,\n",
    "    batch_size=50,\n",
    "    symmetric_normalization=False,\n",
    ")\n",
    "\n",
    "test_gen = gen.flow(\n",
    "    list(test_graphs.index - 1),\n",
    "    targets=test_graphs.values,\n",
    "    batch_size=1,\n",
    "    symmetric_normalization=False,\n",
    ")\n",
    "#training the model\n",
    "history = model_2.fit(\n",
    "    train_gen, epochs=epochs, verbose=1, validation_data=test_gen, shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4638757",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
